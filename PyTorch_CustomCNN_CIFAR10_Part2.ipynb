{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR, CyclicLR, OneCycleLR, StepLR\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2345\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Wanbd Login**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpiyushrs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login to W&B\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Folder declaration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = Path('u:\\\\OneDrive - The University of Texas at Dallas\\\\6382\\\\Datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = base_folder/'data'\n",
    "archive_folder = base_folder/'archive'\n",
    "model_folder = base_folder/'models/CIFAR_10'\n",
    "custom_functions = base_folder/'custom-functions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder.mkdir(exist_ok=True, parents=True)\n",
    "data_folder.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u:\\\\OneDrive - The University of Texas at Dallas\\\\6382\\\\Assignment 6',\n",
       " 'c:\\\\Users\\\\piyus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python310.zip',\n",
       " 'c:\\\\Users\\\\piyus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\DLLs',\n",
       " 'c:\\\\Users\\\\piyus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib',\n",
       " 'c:\\\\Users\\\\piyus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310',\n",
       " '',\n",
       " 'c:\\\\Users\\\\piyus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages',\n",
       " 'c:\\\\Users\\\\piyus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\win32',\n",
       " 'c:\\\\Users\\\\piyus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'c:\\\\Users\\\\piyus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'u:\\\\OneDrive - The University of Texas at Dallas\\\\6382\\\\Datasets\\\\models\\\\CIFAR_10']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append(str(model_folder))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformations and downloading dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([                             \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Download the training_validation data (we will create two subsets - trainset and valset frpm this)\n",
    "train_set = torchvision.datasets.CIFAR10(root = data_folder, \n",
    "                                             train = True, \n",
    "                                             transform = train_transform, \n",
    "                                             download = True)\n",
    "\n",
    "valid_set = torchvision.datasets.CIFAR10(root = data_folder, \n",
    "                                             train = True, \n",
    "                                             transform = valid_transform, \n",
    "                                             download = True)\n",
    "\n",
    "# Download the testing data\n",
    "testset = torchvision.datasets.CIFAR10(root = data_folder, \n",
    "                                            train = False, \n",
    "                                            transform = valid_transform, \n",
    "                                            download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split dataset function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(base_dataset, fraction, seed):\n",
    "    split_a_size = int(fraction * len(base_dataset))\n",
    "    split_b_size = len(base_dataset) - split_a_size\n",
    "    return torch.utils.data.random_split(base_dataset, [split_a_size, split_b_size], \n",
    "                                         generator=torch.Generator().manual_seed(seed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, _ = split_dataset(train_set, 0.8, 42)\n",
    "_, validset = split_dataset(valid_set, 0.8, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 10000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(validset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.dataset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"aqua\"> **Custom CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN_Best(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        super(CIFAR10, self).__init__()\n",
    "\n",
    "        self.conv1_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding='same'), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding='same'), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride = 2 ) \n",
    "        )\n",
    "\n",
    "        self.conv2_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same'), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding='same'), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2) \n",
    "        )\n",
    "\n",
    "        self.conv3_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding='same'), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding='same'), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2) \n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()      \n",
    "        self.fc1 = nn.Linear(16384, out_features=1024)      \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "      \n",
    "        \n",
    "    def forward(self, x):\n",
    "    # conv layers\n",
    "        out = self.conv1_layer(x)\n",
    "        # out = self.conv2_layer(out)\n",
    "        # out = self.conv3_layer(out)\n",
    "\n",
    "        # flatten befrore input to linear layer\n",
    "        out = self.flatten(out)\n",
    "\n",
    "        # linear hidden layers\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.drop1(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.drop2(out)\n",
    "\n",
    "        # output layer - no softmax as it is applied by nn.CrossEntropyLoss\n",
    "\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #########################\n",
    "        ### 1st residual block\n",
    "        #########################\n",
    "        \n",
    "        self.block_1 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=3,\n",
    "                                out_channels=6,\n",
    "                                kernel_size=(1, 1),\n",
    "                                stride=(1, 1),\n",
    "                                padding=0),\n",
    "                torch.nn.BatchNorm2d(6),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv2d(in_channels=6,\n",
    "                                out_channels=3,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(3)\n",
    "        )\n",
    "        \n",
    "        self.block_2 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=3,\n",
    "                                out_channels=6,\n",
    "                                kernel_size=(1, 1),\n",
    "                                stride=(1, 1),\n",
    "                                padding=0),\n",
    "                torch.nn.BatchNorm2d(6),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv2d(in_channels=6,\n",
    "                                out_channels=3,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(3)\n",
    "        )\n",
    "\n",
    "        #########################\n",
    "        ### Fully connected\n",
    "        #########################        \n",
    "        self.linear_1 = torch.nn.Linear(3*32*32, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #########################\n",
    "        ### 1st residual block\n",
    "        #########################\n",
    "        shortcut = x\n",
    "        x = self.block_1(x)\n",
    "        x = torch.nn.functional.relu(x + shortcut)\n",
    "        \n",
    "        #########################\n",
    "        ### 2nd residual block\n",
    "        #########################\n",
    "        shortcut = x\n",
    "        x = self.block_2(x)\n",
    "        x = torch.nn.functional.relu(x + shortcut)\n",
    "        \n",
    "        #########################\n",
    "        ### Fully connected\n",
    "        #########################\n",
    "        logits = self.linear_1(x.view(-1, 3*32*32))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size= 3, padding= 1),               # input => 3 x 32 x 32 output => 32 x 32 x 32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace= True),\n",
    "            nn.Conv2d(32, 32, kernel_size= 3, padding=1, stride= 1),        # 32 x 32 x 32\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace= True),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size= 3, padding= 1, stride= 1),           # 64 x 32 x 32\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace= True),\n",
    "            nn.Conv2d(64, 32, kernel_size= 5, padding=2, stride=1),             # 32 x 32 x 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*16*16, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"aqua\"> **CustomCNN torch summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "              ReLU-2           [-1, 64, 32, 32]               0\n",
      "       BatchNorm2d-3           [-1, 64, 32, 32]             128\n",
      "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-5           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-6           [-1, 64, 16, 16]               0\n",
      "            Conv2d-7          [-1, 128, 16, 16]          73,856\n",
      "              ReLU-8          [-1, 128, 16, 16]               0\n",
      "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
      "           Conv2d-10          [-1, 128, 16, 16]         147,584\n",
      "             ReLU-11          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-12            [-1, 128, 8, 8]               0\n",
      "           Conv2d-13            [-1, 256, 8, 8]         295,168\n",
      "             ReLU-14            [-1, 256, 8, 8]               0\n",
      "      BatchNorm2d-15            [-1, 256, 8, 8]             512\n",
      "           Conv2d-16            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-17            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-18            [-1, 256, 4, 4]               0\n",
      "          Flatten-19                 [-1, 4096]               0\n",
      "           Linear-20                 [-1, 1024]       4,195,328\n",
      "          Dropout-21                 [-1, 1024]               0\n",
      "           Linear-22                  [-1, 512]         524,800\n",
      "          Dropout-23                  [-1, 512]               0\n",
      "           Linear-24                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 5,871,562\n",
      "Trainable params: 5,871,562\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.65\n",
      "Params size (MB): 22.40\n",
      "Estimated Total Size (MB): 27.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(CustomCNN_Best().cuda(), (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, loss_function, model, optimizer, device, grad_clipping, max_norm, log_batch, log_interval):\n",
    "\n",
    "  # Training Loop \n",
    "\n",
    "  # initilalize variables as global\n",
    "  # these counts will be updated every epoch\n",
    "  global batch_ct_train\n",
    "\n",
    "  # Initialize train_loss at the he start of the epoch\n",
    "  running_train_loss = 0\n",
    "  running_train_correct = 0\n",
    "  \n",
    "  # put the model in training mode\n",
    "\n",
    "  model.train()\n",
    "  # Iterate on batches from the dataset using train_loader\n",
    "  for input_, targets in train_loader:\n",
    "    \n",
    "    # move inputs and outputs to GPUs\n",
    "    input_ = input_.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "\n",
    "    # Step 1: Forward Pass: Compute model's predictions \n",
    "    output = model(input_)\n",
    "    \n",
    "    # Step 2: Compute loss\n",
    "    loss = loss_function(output, targets)\n",
    "\n",
    "    # Correct prediction\n",
    "    y_pred = torch.argmax(output, dim = 1)\n",
    "    correct = torch.sum(y_pred == targets)\n",
    "\n",
    "    batch_ct_train += 1\n",
    "\n",
    "    # Step 3: Backward pass -Compute the gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient Clipping\n",
    "    if grad_clipping:\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm, norm_type=2)\n",
    "\n",
    "    # Step 4: Update the parameters\n",
    "    optimizer.step()\n",
    "          \n",
    "    # Add train loss of a batch \n",
    "    running_train_loss += loss.item()\n",
    "\n",
    "    # Add Corect counts of a batch\n",
    "    running_train_correct += correct\n",
    "\n",
    "    # log batch loss and accuracy\n",
    "    if log_batch:\n",
    "      if ((batch_ct_train + 1) % log_interval) == 0:\n",
    "        wandb.log({f\"Train Batch Loss  :\": loss})\n",
    "        wandb.log({f\"Train Batch Acc :\": correct/len(targets)})\n",
    "\n",
    "  \n",
    "  # Calculate mean train loss for the whole dataset for a particular epoch\n",
    "  train_loss = running_train_loss/len(train_loader)\n",
    "\n",
    "  # Calculate accuracy for the whole dataset for a particular epoch\n",
    "  train_acc = running_train_correct/len(train_loader.dataset)\n",
    "  \n",
    "\n",
    "  return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validate Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_loader, loss_function, model, device, log_batch, log_interval):\n",
    "\n",
    "  # initilalize variables as global\n",
    "  # these counts will be updated every epoch\n",
    "  global batch_ct_valid\n",
    "\n",
    "  # Validation/Test loop\n",
    "  # Initialize valid_loss at the he strat of the epoch\n",
    "  running_val_loss = 0\n",
    "  running_val_correct = 0\n",
    "\n",
    "  # put the model in evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for input_,targets in valid_loader:\n",
    "\n",
    "      # move inputs and outputs to GPUs\n",
    "      input_ = input_.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      # Step 1: Forward Pass: Compute model's predictions \n",
    "      output = model(input_)\n",
    "\n",
    "      # Step 2: Compute loss\n",
    "      loss = loss_function(output, targets)\n",
    "\n",
    "      # Correct Predictions\n",
    "      y_pred = torch.argmax(output, dim = 1)\n",
    "      correct = torch.sum(y_pred == targets)\n",
    "\n",
    "      batch_ct_valid += 1\n",
    "\n",
    "      # Add val loss of a batch \n",
    "      running_val_loss += loss.item()\n",
    "\n",
    "      # Add correct count for each batch\n",
    "      running_val_correct += correct\n",
    "\n",
    "      # log batch loss and accuracy\n",
    "      if log_batch:\n",
    "        if ((batch_ct_valid + 1) % log_interval) == 0:\n",
    "          wandb.log({f\"Valid Batch Loss  :\": loss})\n",
    "          wandb.log({f\"Valid Batch Accuracy :\": correct/len(targets)})\n",
    "\n",
    "    # Calculate mean val loss for the whole dataset for a particular epoch\n",
    "    val_loss = running_val_loss/len(valid_loader)\n",
    "\n",
    "    # Calculate accuracy for the whole dataset for a particular epoch\n",
    "    val_acc = running_val_correct/len(valid_loader.dataset)\n",
    "\n",
    "    # scheduler step\n",
    "    # scheduler.step(val_loss)\n",
    "    # scheduler.step()\n",
    "    \n",
    "  return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Loop Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader, valid_loader, model, optimizer, loss_function, epochs, device, patience, early_stopping,\n",
    "               file_model, save_best_model):\n",
    "    \n",
    "  \"\"\" \n",
    "  Function for training the model and plotting the graph for train & validation loss vs epoch.\n",
    "  Input: iterator for train dataset, initial weights and bias, epochs, learning rate, batch size.\n",
    "  Output: final weights, bias and train loss and validation loss for each epoch.\n",
    "  \"\"\"\n",
    "\n",
    "  # Create lists to store train and val loss at each epoch\n",
    "  train_loss_history = []\n",
    "  valid_loss_history = []\n",
    "  train_acc_history = []\n",
    "  valid_acc_history = []\n",
    "\n",
    "  # initialize variables for early stopping\n",
    "\n",
    "  delta = 0\n",
    "  best_score = None\n",
    "  valid_loss_min = np.Inf\n",
    "  counter_early_stop=0\n",
    "  early_stop=False\n",
    "\n",
    "  # Iterate for the given number of epochs\n",
    "  # Step 5: Repeat steps 1 - 4\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    # Get train loss and accuracy for one epoch\n",
    "    train_loss, train_acc = train(train_loader, loss_function, model, optimizer,\n",
    "                                  wandb.config.device, wandb.config.grad_clipping, \n",
    "                                  wandb.config.max_norm, wandb.config.log_batch, wandb.config.log_interval)\n",
    "    valid_loss, valid_acc   = validate(valid_loader, loss_function, model, wandb.config.device, wandb.config.log_batch, wandb.config.log_interval)\n",
    "\n",
    "    dt = datetime.now() - t0\n",
    "\n",
    "    # Save history of the Losses and accuracy\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    valid_loss_history.append(valid_loss)\n",
    "    valid_acc_history.append(valid_acc)\n",
    "\n",
    "    # Log the train and valid loss to wandb\n",
    "    wandb.log({f\"Train Loss :\": train_loss, \"epoch\": epoch})\n",
    "    wandb.log({f\"Train Acc :\": train_acc, \"epoch\": epoch})\n",
    "\n",
    "    wandb.log({f\"Valid Loss :\": valid_loss, \"epoch\": epoch})\n",
    "    wandb.log({f\"Valid Acc :\": valid_acc, \"epoch\": epoch})\n",
    "\n",
    "    if early_stopping:\n",
    "      score = -valid_loss\n",
    "      if best_score is None:\n",
    "        best_score=score\n",
    "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n",
    "        torch.save(model.state_dict(), file_model)\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "      elif score < best_score + delta:\n",
    "        counter_early_stop += 1\n",
    "        print(f'Early stoping counter: {counter_early_stop} out of {patience}')\n",
    "        if counter_early_stop > patience:\n",
    "          early_stop = True\n",
    "\n",
    "      \n",
    "      else:\n",
    "        best_score = score\n",
    "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), file_model)\n",
    "        counter_early_stop=0\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "      if early_stop:\n",
    "        print('Early Stopping')\n",
    "        break\n",
    "\n",
    "    elif save_best_model:\n",
    "\n",
    "      score = -valid_loss\n",
    "      if best_score is None:\n",
    "        best_score=score\n",
    "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n",
    "        torch.save(model.state_dict(), file_model)\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "      elif score < best_score + delta:\n",
    "        print(f'Validation loss has not decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Not Saving Model...')\n",
    "      \n",
    "      else:\n",
    "        best_score = score\n",
    "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), file_model)\n",
    "        valid_loss_min = valid_loss\n",
    "        \n",
    "    else:\n",
    "        torch.save(model.state_dict(), file_model)\n",
    "    \n",
    "    # Print the train loss and accuracy for given number of epochs, batch size and number of samples\n",
    "    print(f'Epoch : {epoch+1} / {epochs}')\n",
    "    print(f'Time to complete {epoch+1} is {dt}')\n",
    "    # print(f'Learning rate: {scheduler._last_lr[0]}')\n",
    "    print(f'Train Loss: {train_loss : .4f} | Train Accuracy: {train_acc * 100 : .4f}%')\n",
    "    print(f'Valid Loss: {valid_loss : .4f} | Valid Accuracy: {valid_acc * 100 : .4f}%')\n",
    "    print()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  return train_loss_history, train_acc_history, valid_loss_history, valid_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Get accuracy prediction function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_pred(data_loader, model, device):\n",
    "    \n",
    "  \"\"\" \n",
    "  Function to get predictions and accuracy for a given data using estimated model\n",
    "  Input: Data iterator, Final estimated weoights, bias\n",
    "  Output: Prections and Accuracy for given dataset\n",
    "  \"\"\"\n",
    "\n",
    "  # Array to store predicted labels\n",
    "  predictions = torch.Tensor() # empty tensor\n",
    "  predictions = predictions.to(device) # move predictions to GPU\n",
    "\n",
    "  # Array to store actual labels\n",
    "  y = torch.Tensor() # empty tensor\n",
    "  y = y.to(device)\n",
    "\n",
    "  # put the model in evaluation mode\n",
    "  model.eval()\n",
    "  \n",
    "  # Iterate over batches from data iterator\n",
    "  with torch.no_grad():\n",
    "    for input_, targets in data_loader:\n",
    "      \n",
    "      # move inputs and outputs to GPUs\n",
    "      \n",
    "      input_ = input_.to(device)\n",
    "      targets = targets.to(device)\n",
    "      \n",
    "      # Calculated the predicted labels\n",
    "      output = model(input_)\n",
    "\n",
    "      # Choose the label with maximum probability\n",
    "      prediction = torch.argmax(output, dim = 1)\n",
    "\n",
    "      # Add the predicted labels to the array\n",
    "      predictions = torch.cat((predictions, prediction)) \n",
    "\n",
    "      # Add the actual labels to the array\n",
    "      y = torch.cat((y, targets)) \n",
    "\n",
    "  # Check for complete dataset if actual and predicted labels are same or not\n",
    "  # Calculate accuracy\n",
    "  acc = (predictions == y).float().mean()\n",
    "\n",
    "  # Return tuple containing predictions and accuracy\n",
    "  return predictions, acc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Meta Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = SimpleNamespace(\n",
    "    epochs = 10,\n",
    "    output_dim = 10,\n",
    "    batch_size= 256,\n",
    "    learning_rate=0.005,\n",
    "    dataset=\"CIFAR10\",\n",
    "    architecture=\"CustomCNN\",\n",
    "    log_interval = 25,\n",
    "    log_batch = True,\n",
    "    file_model = model_folder/'assignment_partB_customCNN_CIFAR10.pt',\n",
    "    grad_clipping = False,\n",
    "    max_norm = 1,\n",
    "    momentum = 0.9,\n",
    "    patience = 5,\n",
    "    early_stopping = False,\n",
    "    scheduler_factor = 0,\n",
    "    scheduler_patience = 0,\n",
    "    weight_decay = 0.00001,\n",
    "    save_best_model = True,\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>u:\\OneDrive - The University of Texas at Dallas\\6382\\Assignment 6\\wandb\\run-20221111_010450-24mk99qt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/piyushrs/CustomCNN11_HW6_B/runs/24mk99qt\" target=\"_blank\">exp13</a></strong> to <a href=\"https://wandb.ai/piyushrs/CustomCNN11_HW6_B\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/piyushrs/CustomCNN11_HW6_B/runs/24mk99qt?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2520a280ca0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(name = \"exp13\", project = 'CustomCNN11_HW6_B' , config = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 10, 'output_dim': 10, 'batch_size': 256, 'learning_rate': 0.005, 'dataset': 'CIFAR10', 'architecture': 'CustomCNN', 'log_interval': 25, 'log_batch': True, 'file_model': 'u:\\\\OneDrive - The University of Texas at Dallas\\\\6382\\\\Datasets\\\\models\\\\CIFAR_10\\\\assignment_partB_customCNN_CIFAR10.pt', 'grad_clipping': False, 'max_norm': 1, 'momentum': 0.9, 'patience': 5, 'early_stopping': False, 'scheduler_factor': 0, 'scheduler_patience': 0, 'weight_decay': 1e-05, 'save_best_model': True, 'device': 'cuda:0'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=wandb.config.batch_size, shuffle = True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=wandb.config.batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=wandb.config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Init weights with Kaiming initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(layer):\n",
    "  if type(layer) == nn.Linear:\n",
    "    torch.nn.init.kaiming_normal_(layer.weight)\n",
    "    torch.nn.init.zeros_(layer.bias)\n",
    "  \n",
    "  if type(layer) == nn.Conv2d:\n",
    "    torch.nn.init.kaiming_normal_(layer.weight)\n",
    "    torch.nn.init.zeros_(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight1(layer):\n",
    "  if type(layer) == nn.Linear:\n",
    "    torch.nn.init.xavier_normal_(layer.weight)\n",
    "    torch.nn.init.zeros_(layer.bias)\n",
    "  \n",
    "  if type(layer) == nn.Conv2d:\n",
    "    torch.nn.init.xavier_normal_(layer.weight)\n",
    "    torch.nn.init.zeros_(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomCNN_Best()\n",
    "model.to(wandb.config.device)\n",
    "# model.apply(init_weight)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_function.to(wandb.config.device)\n",
    "# optim = torch.optim.Adam(model.parameters(), lr = wandb.config.learning_rate, weight_decay=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size= 30, gamma=0.1)\n",
    "# optim = torch.optim.Adagrad(model.parameters(), lr=wandb.config.learning_rate, weight_decay= 0.00001)\n",
    "# optim = torch.optim.RMSprop(model.parameters(), lr=wandb.config.learning_rate, momentum= wandb.config.momentum, weight_decay= wandb.config.weight_decay)\n",
    "optim = torch.optim.SGD(model.parameters(), lr = wandb.config.learning_rate, momentum= wandb.config.momentum, weight_decay= wandb.config.weight_decay, nesterov= False)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=0.15, total_steps=len(train_loader) * wandb.config.epochs, epochs=wandb.config.epochs, three_phase=True)\n",
    "\n",
    "# scheduler = ReduceLROnPlateau(optim, mode='min', factor= wandb.config.scheduler_factor, patience=wandb.config.scheduler_patience, verbose=True)\n",
    "\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"Red\"> **Training Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss has decreased (inf --> 1.117806). Saving Model...\n",
      "Epoch : 1 / 10\n",
      "Time to complete 1 is 0:00:22.927882\n",
      "Train Loss:  1.5089 | Train Accuracy:  44.1750%\n",
      "Valid Loss:  1.1178 | Valid Accuracy:  59.6200%\n",
      "\n",
      "Validation loss has decreased (1.117806 --> 0.847781). Saving model...\n",
      "Epoch : 2 / 10\n",
      "Time to complete 2 is 0:00:22.009028\n",
      "Train Loss:  0.9682 | Train Accuracy:  65.5800%\n",
      "Valid Loss:  0.8478 | Valid Accuracy:  69.7500%\n",
      "\n",
      "Validation loss has decreased (0.847781 --> 0.780075). Saving model...\n",
      "Epoch : 3 / 10\n",
      "Time to complete 3 is 0:00:21.765638\n",
      "Train Loss:  0.7506 | Train Accuracy:  73.6425%\n",
      "Valid Loss:  0.7801 | Valid Accuracy:  72.5300%\n",
      "\n",
      "Validation loss has decreased (0.780075 --> 0.732226). Saving model...\n",
      "Epoch : 4 / 10\n",
      "Time to complete 4 is 0:00:21.811886\n",
      "Train Loss:  0.6191 | Train Accuracy:  78.0225%\n",
      "Valid Loss:  0.7322 | Valid Accuracy:  74.3400%\n",
      "\n",
      "Validation loss has decreased (0.732226 --> 0.673304). Saving model...\n",
      "Epoch : 5 / 10\n",
      "Time to complete 5 is 0:00:22.350078\n",
      "Train Loss:  0.4965 | Train Accuracy:  82.5625%\n",
      "Valid Loss:  0.6733 | Valid Accuracy:  76.7600%\n",
      "\n",
      "Validation loss has decreased (0.673304 --> 0.611086). Saving model...\n",
      "Epoch : 6 / 10\n",
      "Time to complete 6 is 0:00:21.287759\n",
      "Train Loss:  0.4073 | Train Accuracy:  85.5850%\n",
      "Valid Loss:  0.6111 | Valid Accuracy:  79.2100%\n",
      "\n",
      "Validation loss has not decreased (0.611086 --> 0.652738). Not Saving Model...\n",
      "Epoch : 7 / 10\n",
      "Time to complete 7 is 0:00:21.125033\n",
      "Train Loss:  0.3189 | Train Accuracy:  88.6475%\n",
      "Valid Loss:  0.6527 | Valid Accuracy:  79.5800%\n",
      "\n",
      "Validation loss has not decreased (0.611086 --> 0.709267). Not Saving Model...\n",
      "Epoch : 8 / 10\n",
      "Time to complete 8 is 0:00:21.054387\n",
      "Train Loss:  0.2438 | Train Accuracy:  91.4375%\n",
      "Valid Loss:  0.7093 | Valid Accuracy:  79.9200%\n",
      "\n",
      "Validation loss has not decreased (0.611086 --> 0.651456). Not Saving Model...\n",
      "Epoch : 9 / 10\n",
      "Time to complete 9 is 0:00:21.040999\n",
      "Train Loss:  0.1712 | Train Accuracy:  93.9600%\n",
      "Valid Loss:  0.6515 | Valid Accuracy:  81.5000%\n",
      "\n",
      "Validation loss has not decreased (0.611086 --> 0.679758). Not Saving Model...\n",
      "Epoch : 10 / 10\n",
      "Time to complete 10 is 0:00:21.021151\n",
      "Train Loss:  0.1328 | Train Accuracy:  95.1275%\n",
      "Valid Loss:  0.6798 | Valid Accuracy:  80.9300%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See live graphs in the notebook.\n",
    "#%%wandb \n",
    "batch_ct_train, batch_ct_valid = 0, 0\n",
    "train_loss_history, train_acc_history, valid_loss_history, valid_acc_history = train_loop(train_loader, \n",
    "                                                                                          valid_loader, \n",
    "                                                                                          model, \n",
    "                                                                                          optim, \n",
    "                                                                                          loss_function, \n",
    "                                                                                          wandb.config.epochs, \n",
    "                                                                                          wandb.config.device,\n",
    "                                                                                          wandb.config.patience,\n",
    "                                                                                          wandb.config.early_stopping,\n",
    "                                                                                          wandb.config.file_model,\n",
    "                                                                                          wandb.config.save_best_model\n",
    "                                                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e361d1ca83441b8dad78917f288aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Acc :</td><td>▁▄▅▆▆▇▇▇██</td></tr><tr><td>Train Batch Acc :</td><td>▁▂▃▄▅▅▅▅▅▆▆▆▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇█▇█▇████████</td></tr><tr><td>Train Batch Loss  :</td><td>█▇▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Loss :</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>Valid Acc :</td><td>▁▄▅▆▆▇▇▇██</td></tr><tr><td>Valid Batch Accuracy :</td><td>▁▂▂▃▅▆▅▅▆▇▅▅▆█▇▇</td></tr><tr><td>Valid Batch Loss  :</td><td>█▆▇▆▄▃▄▄▂▁▄▇▅▁▄▂</td></tr><tr><td>Valid Loss :</td><td>█▄▃▃▂▁▂▂▂▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Acc :</td><td>0.95127</td></tr><tr><td>Train Batch Acc :</td><td>0.91406</td></tr><tr><td>Train Batch Loss  :</td><td>0.19608</td></tr><tr><td>Train Loss :</td><td>0.13283</td></tr><tr><td>Valid Acc :</td><td>0.8093</td></tr><tr><td>Valid Batch Accuracy :</td><td>0.81641</td></tr><tr><td>Valid Batch Loss  :</td><td>0.57806</td></tr><tr><td>Valid Loss :</td><td>0.67976</td></tr><tr><td>epoch</td><td>9</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">exp13</strong>: <a href=\"https://wandb.ai/piyushrs/CustomCNN11_HW6_B/runs/24mk99qt\" target=\"_blank\">https://wandb.ai/piyushrs/CustomCNN11_HW6_B/runs/24mk99qt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221111_010450-24mk99qt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa6fc890c19d0518efc3ca8fefe1ab7c2a921b84ac247755bf14d3f8c1bb1ece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
